{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83455744",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+--------------------+\n",
      "|    subject|        relation|              object|\n",
      "+-----------+----------------+--------------------+\n",
      "|wsdbm:City0|gn:parentCountry|     wsdbm:Country20|\n",
      "|wsdbm:City1|gn:parentCountry|      wsdbm:Country0|\n",
      "|wsdbm:City2|gn:parentCountry|      wsdbm:Country1|\n",
      "|wsdbm:City3|gn:parentCountry|      wsdbm:Country6|\n",
      "|wsdbm:City4|gn:parentCountry|     wsdbm:Country15|\n",
      "|wsdbm:City5|gn:parentCountry|      wsdbm:Country1|\n",
      "|wsdbm:City6|gn:parentCountry|     wsdbm:Country11|\n",
      "|wsdbm:City7|gn:parentCountry|      wsdbm:Country3|\n",
      "|wsdbm:City8|gn:parentCountry|      wsdbm:Country1|\n",
      "|wsdbm:City9|gn:parentCountry|      wsdbm:Country0|\n",
      "|wsdbm:User0|      sorg:email|\"waxedinglesonarc...|\n",
      "|wsdbm:User0|    wsdbm:userId|           \"1806723\"|\n",
      "|wsdbm:User0|   wsdbm:follows|        wsdbm:User24|\n",
      "|wsdbm:User0|   wsdbm:follows|        wsdbm:User27|\n",
      "|wsdbm:User0|   wsdbm:follows|        wsdbm:User37|\n",
      "|wsdbm:User0|   wsdbm:follows|       wsdbm:User110|\n",
      "|wsdbm:User0|   wsdbm:follows|       wsdbm:User148|\n",
      "|wsdbm:User0|   wsdbm:follows|       wsdbm:User219|\n",
      "|wsdbm:User0|   wsdbm:follows|       wsdbm:User265|\n",
      "|wsdbm:User0|   wsdbm:follows|       wsdbm:User276|\n",
      "+-----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------------+\n",
      "|      subject|          object|\n",
      "+-------------+----------------+\n",
      "| wsdbm:Offer0|wsdbm:Product210|\n",
      "| wsdbm:Offer1|wsdbm:Product200|\n",
      "| wsdbm:Offer2|wsdbm:Product165|\n",
      "| wsdbm:Offer3|wsdbm:Product230|\n",
      "| wsdbm:Offer4|  wsdbm:Product2|\n",
      "| wsdbm:Offer5|wsdbm:Product203|\n",
      "| wsdbm:Offer6| wsdbm:Product16|\n",
      "| wsdbm:Offer7| wsdbm:Product70|\n",
      "| wsdbm:Offer8|wsdbm:Product220|\n",
      "| wsdbm:Offer9|wsdbm:Product116|\n",
      "|wsdbm:Offer10|wsdbm:Product233|\n",
      "|wsdbm:Offer11|wsdbm:Product186|\n",
      "|wsdbm:Offer12|wsdbm:Product212|\n",
      "|wsdbm:Offer13|wsdbm:Product102|\n",
      "|wsdbm:Offer14|wsdbm:Product179|\n",
      "|wsdbm:Offer15|wsdbm:Product193|\n",
      "|wsdbm:Offer16| wsdbm:Product25|\n",
      "|wsdbm:Offer17| wsdbm:Product25|\n",
      "|wsdbm:Offer18| wsdbm:Product90|\n",
      "|wsdbm:Offer19| wsdbm:Product58|\n",
      "+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "#import tracemalloc\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "#Imports\n",
    "from pyspark.sql import functions as psfunctions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split, udf, desc, concat, col, lit\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "#Initialise session\n",
    "spark = SparkSession.builder.appName(\"cite\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlc = SQLContext(sc)\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "#load the ralations into df\n",
    "relation_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"100k.txt\")\n",
    "\n",
    "#Rename columns\n",
    "relation_df = relation_df.select(col(\"_c0\").alias(\"subject\"), col(\"_c1\").alias(\"relation\"), col(\"_c2\").alias(\"object\"))\n",
    "relation_df = relation_df.withColumn(\"object\",psfunctions.regexp_replace('object', ' .', ''))\n",
    "relation_df.show()\n",
    "\n",
    "#Count how many relations there are\n",
    "relation_index_df = relation_df.select(\"relation\").distinct()\n",
    "relation_index_df = relation_index_df.withColumn(\"index\", psfunctions.row_number().over(Window.orderBy(\"relation\")))\n",
    "\n",
    "#Creating a list containing the different df\n",
    "relation_list = relation_index_df.select(\"relation\").toPandas()\n",
    "relation_list = list(relation_list['relation'])\n",
    "relation_df_list = list(map(lambda rel : relation_df.filter(relation_df.relation == rel), relation_list))\n",
    "relation_dict = dict()\n",
    "relation_dict_reversed = dict()\n",
    "for i,x in enumerate(relation_list):\n",
    "    relation_dict[x] = i\n",
    "    relation_dict_reversed[i] = x\n",
    "\n",
    "#Creating a dict for all subject and objects\n",
    "objects_list = relation_df.select(\"object\").toPandas()\n",
    "objects_list = list(objects_list[\"object\"])\n",
    "subjects_list = relation_df.select(\"subject\").toPandas()\n",
    "subjects_list = list(subjects_list[\"subject\"])\n",
    "so_list = objects_list + subjects_list\n",
    "so_set = set(so_list)\n",
    "\n",
    "#Index -> SO\n",
    "so_dict = dict()\n",
    "\n",
    "#SO -> Index\n",
    "so_dict_reverse = dict()\n",
    "\n",
    "for i, x in enumerate(so_set):\n",
    "    so_dict_reverse[x] = i\n",
    "    so_dict[i] = x\n",
    "\n",
    "map_col = psfunctions.create_map([psfunctions.lit(x) for i in so_dict_reverse.items() for x in i])\n",
    "relation_df_list_numbers = list(map(lambda df : df.withColumn(\"subject\", map_col[psfunctions.col('subject')].cast(\"int\")).withColumn('object', map_col[psfunctions.col('object')].cast(\"int\")), relation_df_list))\n",
    "\n",
    "#Dropping the relation column, as it is superfluous\n",
    "relation_df_list = list(map(lambda df : df.drop(\"relation\"), relation_df_list))\n",
    "relation_df_list_numbers = list(map(lambda df : df.drop(\"relation\"), relation_df_list_numbers))\n",
    "relation_df_list[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191d4b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###HashJoin\n",
    "\n",
    "#Imports\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "def hashJoin(df1, key1, df2, key2):\n",
    "    \n",
    "    #Setup dict that stores all rows according to their key\n",
    "    hash_map = defaultdict(list)\n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    for index, row in df1.iterrows():\n",
    "        hash_map[row[key1]].append(list(row))\n",
    "    \n",
    "    #Defining column names for the returned/constructed df \n",
    "    #EXAMPLE: [subject, 1, 2, 3, ..., object]\n",
    "    #Thus we can garantee multiple consecutive joines with joines on the (object, subject) pair\n",
    "    col_list = df1.columns.tolist()\n",
    "    col_list2 = df2.columns.tolist()\n",
    "    col_list= col_list + col_list2\n",
    "    col_len = len(col_list)\n",
    "    col_list = [str(i) for i in range(0, col_len-1)]\n",
    "    col_list[col_len-2] = \"object\"\n",
    "    col_list[0] = \"subject\"\n",
    "    \n",
    "    #Calculate needed size for joined df\n",
    "    counter = 0\n",
    "    t1 = time.time()\n",
    "    for index, row in df2.iterrows():\n",
    "        for h in hash_map[row[key2]]:\n",
    "            counter += 1\n",
    "    \n",
    "    #Creating temporary/join df that will be filled with the joined rows\n",
    "    tmp_df = pd.DataFrame(index=np.arange(counter), columns = col_list)\n",
    "    \n",
    "    #Joining different rows by iterating trough the second df and finding and linking the matched rows in the hashmap\n",
    "    row_counter = 0\n",
    "    for index, row in df2.iterrows():\n",
    "        t1 = time.time()\n",
    "        for h in hash_map[row[key2]]:\n",
    "            c = copy.deepcopy(h)\n",
    "            c.pop()\n",
    "            c = c + list(row)\n",
    "            tmp_df.loc[row_counter] = c\n",
    "            row_counter += 1\n",
    "    return spark.createDataFrame(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa1967b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Join took  1.220749855041504 seconds\n",
      "Second Join took  29.75323724746704 seconds\n",
      "Third Join took  949.3976566791534 seconds\n",
      "+-------------+------------+------------+--------------+--------------+\n",
      "|      subject|           1|           2|             3|        object|\n",
      "+-------------+------------+------------+--------------+--------------+\n",
      "|wsdbm:User630| wsdbm:User9|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User26|wsdbm:User57|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User197|wsdbm:User57|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User351|wsdbm:User57|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User396|wsdbm:User57|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User704|wsdbm:User57|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User173|wsdbm:User67|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User95|wsdbm:User81|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User522|wsdbm:User81|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User610|wsdbm:User81|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User999|wsdbm:User81|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User32|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User44|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User67|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User84|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "| wsdbm:User88|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User151|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User159|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User160|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "|wsdbm:User194|wsdbm:User88|wsdbm:User20|wsdbm:Product0|wsdbm:Review24|\n",
      "+-------------+------------+------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performing the requested SQL expression with HASHJOIN from right to left\n",
    "#Takes approx 30 Mins on i7-7700hq for 100k.txt\n",
    "t1 = time.time()\n",
    "likes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list[relation_dict[\"rev:hasReview\"]], \"subject\")\n",
    "print(\"First Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "friends_likes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\n",
    "print(\"Second Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "follows_friends_likes_hasreview_reversed = hashJoin(relation_df_list[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\n",
    "print(\"Third Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "follows_friends_likes_hasreview_reversed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cf24ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Performing the requested SQL expression with HASHJOIN from right to left\\n#Takes approx 30 Mins on i7-7700hq for 100k.txt\\ntracemalloc.start()\\nt1 = time.time()\\nlikes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list[relation_dict[\"rev:hasReview\"]], \"subject\")\\nprint(\"First Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\n\\ntracemalloc.start()\\nt1 = time.time()\\nfriends_likes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\\nprint(\"Second Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\n\\ntracemalloc.start()\\nt1 = time.time()\\nfollows_friends_likes_hasreview_reversed = hashJoin(relation_df_list[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\\nprint(\"Third Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\nfollows_friends_likes_hasreview_reversed.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Performing the requested SQL expression with HASHJOIN from right to left\n",
    "#Takes approx 30 Mins on i7-7700hq for 100k.txt\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "likes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list[relation_dict[\"rev:hasReview\"]], \"subject\")\n",
    "print(\"First Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "friends_likes_hasreview = hashJoin(relation_df_list[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\n",
    "print(\"Second Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "follows_friends_likes_hasreview_reversed = hashJoin(relation_df_list[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\n",
    "print(\"Third Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "follows_friends_likes_hasreview_reversed.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e33d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Performing the requested SQL expression with HASHJOIN from left to right\\n#Takes much longer than the above solution\\n\\nt1 = time.time()\\nfollows_friends = hashJoin(relation_df_list[relation_dict[\"wsdbm:follows\"]], \"object\", relation_df_list[relation_dict[\"wsdbm:friendOf\"]], \"subject\")\\nprint(\"First Join took \", time.time()-t1, \"seconds\")\\n\\nt1 = time.time()\\nfollows_friends_likes = hashJoin(follows_friends, \"object\", relation_df_list[relation_dict[\"wsdbm:likes\"]], \"subject\")\\nprint(\"Second Join took \", time.time()-t1, \"seconds\")\\n\\nt1 = time.time()\\nfollows_friends_likes_hasreview = hashJoin(follows_friends_likes, \"object\", relation_df_list[relation_dict[\"rev:hasReview\"]], \"subject\")\\nprint(\"Third Join took \", time.time()-t1, \"seconds\")\\n\\nfollows_friends_likes_hasreview.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Performing the requested SQL expression with HASHJOIN from left to right\n",
    "#Takes much longer than the above solution\n",
    "\n",
    "t1 = time.time()\n",
    "follows_friends = hashJoin(relation_df_list[relation_dict[\"wsdbm:follows\"]], \"object\", relation_df_list[relation_dict[\"wsdbm:friendOf\"]], \"subject\")\n",
    "print(\"First Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "follows_friends_likes = hashJoin(follows_friends, \"object\", relation_df_list[relation_dict[\"wsdbm:likes\"]], \"subject\")\n",
    "print(\"Second Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "follows_friends_likes_hasreview = hashJoin(follows_friends_likes, \"object\", relation_df_list[relation_dict[\"rev:hasReview\"]], \"subject\")\n",
    "print(\"Third Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "follows_friends_likes_hasreview.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2cf92b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###SortMergeJoin\n",
    "\n",
    "def sort_merge_join(df1, key1, df2, key2):\n",
    "\n",
    "    #Sorting the left and right df\n",
    "    df1_sorted = df1.sort(key1, key2)\n",
    "    df2_sorted = df2.sort(key2, key1)\n",
    "    \n",
    "    left_index = 0\n",
    "    right_index = 0\n",
    "    \n",
    "    #Using pandas to modify and iterate the df\n",
    "    df1_pandas = df1_sorted.toPandas()\n",
    "    df2_pandas = df2_sorted.toPandas()\n",
    "    \n",
    "    #Getting the df row lengths\n",
    "    df1_row_len = len(df1_pandas.index)\n",
    "    df2_row_len = len(df2_pandas.index)\n",
    "\n",
    "    #Determining the size of the temporary/joined df\n",
    "    counter = 0\n",
    "    while right_index < df2_row_len and left_index < df1_row_len:\n",
    "        key = min(df2_pandas.loc[right_index].at[key2], df1_pandas.loc[left_index].at[key1])\n",
    "        r_counter = 0\n",
    "        l_counter = 0\n",
    "        while right_index < df2_row_len and key == df2_pandas.loc[right_index].at[key2]:\n",
    "            r_counter += 1\n",
    "            right_index += 1\n",
    "        while left_index < df1_row_len and key == df1_pandas.loc[left_index].at[key1]:\n",
    "            l_counter += 1\n",
    "            left_index += 1\n",
    "        counter += l_counter * r_counter\n",
    "        \n",
    "    #Defining column names for the returned/constructed df \n",
    "    #EXAMPLE: [subject, 1, 2, 3, ..., object]\n",
    "    #Thus we can garantee multiple consecutive joines with joines on the (object, subject) pair\n",
    "    col_list = df1.columns\n",
    "    col_list2 = df2.columns\n",
    "    col_list= col_list + col_list2\n",
    "    col_len = len(col_list)\n",
    "    col_list = [str(i) for i in range(0, col_len-1)]\n",
    "    col_list[col_len-2] = \"object\"\n",
    "    col_list[0] = \"subject\"\n",
    "    \n",
    "    #Creating temporary/join df that will be filled with the joined rows\n",
    "    tmp_df = pd.DataFrame(index=np.arange(counter), columns = col_list)\n",
    "    \n",
    "    #Iterating through both sorted df at the same time and joining rows when there are key matches\n",
    "    left_index = 0\n",
    "    right_index = 0\n",
    "    row_counter = 0\n",
    "    while right_index < df2_row_len and left_index < df1_row_len:\n",
    "        key = min(df2_pandas.loc[right_index].at[key2], df1_pandas.loc[left_index].at[key1])\n",
    "        right_group = []\n",
    "        while right_index < df2_row_len and key == df2_pandas.loc[right_index].at[key2]:\n",
    "            right_group.append(list(df2_pandas.loc[right_index]))\n",
    "            right_index += 1\n",
    "        left_group = []\n",
    "        while left_index < df1_row_len and key == df1_pandas.loc[left_index].at[key1]:\n",
    "            left_group.append(list(df1_pandas.loc[left_index]))\n",
    "            left_index += 1\n",
    "        for i in right_group:\n",
    "            for o in left_group:\n",
    "                c = o[:-1]\n",
    "                c = c + i\n",
    "                tmp_df.loc[row_counter] = c\n",
    "                row_counter += 1\n",
    "    return spark.createDataFrame(tmp_df.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0629d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|subject|  1|object|\n",
      "+-------+---+------+\n",
      "|    245|522|    40|\n",
      "|   6377|522|    40|\n",
      "|   8377|522|    40|\n",
      "|   8948|522|    40|\n",
      "|   9994|522|    40|\n",
      "|  10924|522|    40|\n",
      "|    245|522|    90|\n",
      "|   6377|522|    90|\n",
      "|   8377|522|    90|\n",
      "|   8948|522|    90|\n",
      "|   9994|522|    90|\n",
      "|  10924|522|    90|\n",
      "|    245|522|   127|\n",
      "|   6377|522|   127|\n",
      "|   8377|522|   127|\n",
      "|   8948|522|   127|\n",
      "|   9994|522|   127|\n",
      "|  10924|522|   127|\n",
      "|    245|522|   484|\n",
      "|   6377|522|   484|\n",
      "+-------+---+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "First Join took  2.7328715324401855 seconds\n",
      "Second Join took  30.896827459335327 seconds\n",
      "Third Join took  848.6176447868347 seconds\n",
      "+-------+---+----+-----+------+\n",
      "|subject|  1|   2|    3|object|\n",
      "+-------+---+----+-----+------+\n",
      "|   5397| 22|2876| 3944|     7|\n",
      "|   5867| 22|2876| 3944|     7|\n",
      "|   6082| 22|2876| 3944|     7|\n",
      "|   6310| 22|2876| 3944|     7|\n",
      "|  11975| 22|2876| 3944|     7|\n",
      "|  12310| 22|2876| 3944|     7|\n",
      "|   5397| 22|8323| 9104|    11|\n",
      "|   5867| 22|8323| 9104|    11|\n",
      "|   6082| 22|8323| 9104|    11|\n",
      "|   6310| 22|8323| 9104|    11|\n",
      "|  11975| 22|8323| 9104|    11|\n",
      "|  12310| 22|8323| 9104|    11|\n",
      "|   5397| 22|8323| 9104|    47|\n",
      "|   5867| 22|8323| 9104|    47|\n",
      "|   6082| 22|8323| 9104|    47|\n",
      "|   6310| 22|8323| 9104|    47|\n",
      "|  11975| 22|8323| 9104|    47|\n",
      "|  12310| 22|8323| 9104|    47|\n",
      "|   5397| 22|  25|10754|   133|\n",
      "|   5867| 22|  25|10754|   133|\n",
      "+-------+---+----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performing the requested SQL expression with SORTMERGEJOIN from right to left\n",
    "#Takes approx 25 Mins on i7-7700hq for 100k.txt\n",
    "t1 = time.time()\n",
    "likes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list_numbers[relation_dict[\"rev:hasReview\"]], \"subject\")\n",
    "likes_hasreview.show()\n",
    "print(\"First Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "friends_likes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\n",
    "print(\"Second Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "t1 = time.time()\n",
    "follows_friends_likes_hasreview_reversed = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\n",
    "print(\"Third Join took \", time.time()-t1, \"seconds\")\n",
    "\n",
    "follows_friends_likes_hasreview_reversed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48659542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Performing the requested SQL expression with SORTMERGEJOIN from right to left\\n#Takes approx 25 Mins on i7-7700hq for 100k.txt\\ntracemalloc.start()\\nt1 = time.time()\\nlikes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list_numbers[relation_dict[\"rev:hasReview\"]], \"subject\")\\nprint(\"First Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\n\\ntracemalloc.start()\\nt1 = time.time()\\nfriends_likes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\\nprint(\"Second Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\n\\ntracemalloc.start()\\nt1 = time.time()\\nfollows_friends_likes_hasreview_reversed = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\\nprint(\"Third Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\\ntracemalloc.stop()\\ntracemalloc.reset_peak()\\nfollows_friends_likes_hasreview_reversed.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Performing the requested SQL expression with SORTMERGEJOIN from right to left\n",
    "#Takes approx 25 Mins on i7-7700hq for 100k.txt\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "likes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:likes\"]], \"object\", relation_df_list_numbers[relation_dict[\"rev:hasReview\"]], \"subject\")\n",
    "print(\"First Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "friends_likes_hasreview = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:friendOf\"]], \"object\", likes_hasreview, \"subject\")\n",
    "print(\"Second Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "\n",
    "tracemalloc.start()\n",
    "t1 = time.time()\n",
    "follows_friends_likes_hasreview_reversed = sort_merge_join(relation_df_list_numbers[relation_dict[\"wsdbm:follows\"]], \"object\", friends_likes_hasreview, \"subject\")\n",
    "print(\"Third Join took \", time.time()-t1, \"seconds and \", tracemalloc.get_traced_memory(),\"memory\")\n",
    "tracemalloc.stop()\n",
    "tracemalloc.reset_peak()\n",
    "follows_friends_likes_hasreview_reversed.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356dc63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The advanced Join, resp. task c) of the submission is done in a separate python file as jupyter notebook restricts multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6525cd7-861c-42a7-bcbb-49fa506d4332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
